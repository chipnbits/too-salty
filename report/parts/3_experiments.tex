\section{Experiments}\label{sec:experiments}

\subsection{Method}

We train a baseline model for image classification on the CIFAR-100 dataset with ResNet-50~\cite{resnet50} using a well-known set of hyper-parameters~\cite{cifar100params} with image reflection and random translation with padding for data augmentation. We hold out $5\%$ of the $50,000$ training images as a validation set, as well as $10,000$ images for testing. The baseline model is saved every 10 epochs. From each save point, we branch off and train $4$ new models with different optimizer perturbations. For details, see Appendix Tables~\ref{tab:optimizer_perturbations} and~\ref{tab:hyperparams_baseline}. The perturbed models are each trained to convergence, with the best validation scored model weights saved for the experiments. This process is illustrated in Figure~\ref{fig:evolution}. A total of $4$ variants and $26$ branch points were trained, yielding $104$ related models with $5,346$ binary souping combinations for analysis.

Our experiments are designed to recreate the branching effect of several fine-tuning trajectories, but with a wider variety of models and shared baseline training steps than a single pre-trained model would provide.

\subsection{Shared Epochs and Soup Gain}

We group the soup combinations by the number of shared epochs between the two models before they diverge into different training trajectories. For example, a pair of models branched form the baseline at epoch 50 and 100 respectively share 50 epochs. In Figure~\ref{fig:soup_gain_quantiles}, we plot quantiles of soup gain as a function of shared epochs. We observe the distribution of soup gains shifts positively as the number of shared epochs increases. However, the soup gain is often large and negative until around epoch 150. After epoch 250, nearly all soups are approximately neutral.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/soup_gain_quantiles_vs_shared_epochs.pdf}
    \caption{Quantiles of soup gain vs shared epochs. 
    % As the number of shared epochs increases, the distribution of soup gains shifts positively. However, the median soup gain is negative until around 150 shared epochs. After 250 shared epochs, nearly all soups are approximately neutral.
    }\label{fig:soup_gain_quantiles}
\end{figure}

To better illustrate how often souping works and how much it helps, we plot the probability of positive soup gain and the average soup gain in accuracy for such `positive' soups in Figure~\ref{fig:prob_and_conditional_gain}.The probability of positive soup gain increases with shared epochs, reaching around $80\%$ after 250 shared epochs. The conditional expected soup gain is noisy, at around 0.5\% accuracy improvement when souping works. Towards the upper end of shared epochs, the gains decrease to around 0.2\%. This suggests that while souping becomes more likely to work with increased shared training, the magnitude of gain decreases when the models are too similar.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/combined_prob_and_conditional_gain.pdf}
    \caption{Probability of positive soup gain and expected soup gain given positive soup gain vs shared epochs. 
    % As shared epochs increases, the probability of positive soup gain increases, reaching around $80\%$ after 250 shared epochs. The expected soup gain given positive soup gain is relatively flat at around 0.5\% accuracy improvement, decreasing to around 0.2\% at high shared epochs.
    }\label{fig:prob_and_conditional_gain}
\end{figure}

\subsection{Predicting Soupability with Similarity}

Given two models, can we predict whether or not they will soup? To test this hypothesis, we compute a variety of similarity and distance metrics between pairs of models. This includes the l2 distance and cosine similarity on a vector of the model parameters, the Kullback-Leibler (KL) divergence between the output logits on the test set, the mean squared error (MSE) between the output logits on the test set, and the Centered Kernel Alignment (CKA)~\citep{SimilarityCKA} between the logits and the penultimate layer activations on the test set. 

We find that all metrics perform similarly. A plot for all metrics can be found in Figure~\ref{fig:soup_gain_vs_all_metrics}. We arbitrarily choose to show the KL divergence between the logits in Figure~\ref{fig:soup_gain_vs_kl_logits} as an example. There is a strong correlation between KL divergence and soup gain with a Spearman correlation between of -0.86. However, many of these soups have very poor performance. Figure~\ref{fig:soup_gain_vs_kl_positive_soups} shows the soup gain against KL divergence for only those models with positive soup gain, with a moderate positive correlation with a Spearman correlation of 0.39. Models must be sufficiently similar in order to soup, but to be effective, the models must also be sufficiently differentiated. Balancing these two effects is key to effective souping.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{./figures/soup_gain_vs_kl.png}
  \caption{KL of logits vs soup gain. Spearman correlation of -0.86.
  % There is a strong negative correlation between soup gain and KL divergence. Models must be sufficiently similar for souping to not cause model collapse.
  }\label{fig:soup_gain_vs_kl_logits}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{./figures/soup_gain_vs_kl_positive_soups.png}
  \caption{KL of logits vs soup gain only for positive soups. Spearman correlation of 0.39.
  % There is a moderate positive correlation between soup gain and KL divergence, with a Spearman correlation of 0.39. Notably, the smallest divergences always yield smaller gains, while a little more diversity can lead to larger gains.
  }\label{fig:soup_gain_vs_kl_positive_soups}
\end{figure}

\subsection{Is Souping Transitive?}

If model $A$ soups successfully with models $B$ and $C$, will $B$ and $C$ also soup successfully? If it is true that models soups together when they lie in the same low loss region, then transitivity holds. To test this, we consider all triplets of models $(A, B, C)$. We plot the probability that $B$ and $C$ soup against the number of positive soups involving $A$ in Figure~\ref{fig:transitivity}. We observe that the probability that $B$ and $C$ soup is only high when $A$ soups with both $B$ and $C$. This suggests that souping is approximately transitive.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{./figures/transitivity_binary.pdf}
  \caption{Probability of positive soup gain of models $B$ and $C$ vs the number of positive soups with model $A$.
  % The probability that models $B$ and $C$ soup successfully is low unless $A$ soups successfully with both $B$ and $C$. This suggests that souping is approximately transitive.
  }\label{fig:transitivity}
\end{figure}

We also plot the soup gain between $B$ and $C$ against the minimum soup gain of $(A, B)$ and $(A, C)$ in Figure~\ref{fig:transitivity_min}, noting a moderate positive Spearman correlation of 0.64, further supporting the hypothesis.

Following transitivity, we investigate whether there are clusters of models that soup well together. We embed our models into 2-dimensions based on soup gain and plot them in Figure~\ref{fig:model_embedding}. The details of generating this plot can be found in Section~\ref{sec:model_embedding_creation}. We find that most successful soups lie in a single cluster of models all branched from later on in the training procedure.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{./figures/model_embeddings.pdf}
  \caption{104 initially trained models embedded in 2-dimensions using distance metric based on soup gain. Edges indicate positive soup gain. Color denotes the branching epoch. We see that nearly all successful soups are in the same cluster of models.}\label{fig:model_embedding}
\end{figure}

We conclude that souping is approximately transitive. There are many counter-examples, but in general, this work supports the idea that soups lie in a single loss basin.

\subsection{Souping for Robustness to Corruption}

All experiments thus far have measured soup gain on a held-out test set. However, souping has also been used for robustness to distribution shift~\citep{seasoningsoups}. To establish whether souping for in-distribution performance also increases out-of-distribution performance, we compute the soup gain on CIFAR-100C~\citep{cifar100c} with severity level 3. The soup gains on test and corrupted data correlate very well, with a Spearman correlation of 0.99. A scatterplot can be found in Figure~\ref{fig:test_vs_corrupted}. The positive trend still holds when conditioning on only soups with positive soup gain on the test set, with a Pearson correlation of $0.61$. A plot can be found in Figure~\ref{fig:test_vs_corrupted_pos}. Model gains across the test set and corrupted data are strongly correlated, indicating that source performance improvement transfers to unseen target distributions.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{./figures/soup_test_vs_corrupted_pos_soups.png}
  \caption{Plot of soup gain on test vs corrupted data for only models with positive soup gain on the test set. Spearman correlation of 0.61. 
  % There is a moderate correlation. In general, when souping helps on the test set, it helps on corrupted data.
  }\label{fig:test_vs_corrupted_pos}
\end{figure}

We also plot the probability of positive soup gain on corrupted data as a function of shared epochs in Figure~\ref{fig:soup_corrupted_positive_gain_prob}. The probability of positive gain increases with the number of shared epochs, for both clean and corrupted data. However, the corrupted data consistently has a slightly lower probability of positive gain.

Finally, we test whether souping can help reduce the \emph{robustness gap}, the gap between the test set and corrupted set loss. A mean and median difference in robustness gap before and after souping of approximately, with the CDF plotted in Figure~\ref{fig:soup_robustness_cdf}. We conclude that souping does not reduce the robustness gap.
