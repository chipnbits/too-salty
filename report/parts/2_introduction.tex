\section{Introduction}

\subsection{Motivation}

\citet{soups} introduced the idea of \emph{souping}, where models weights are averaged into a single model. Ingredients for the soup are trained from a shared pre-trained model but undergo different training trajectories. For example, models can be trained with different hyper-parameters or with different data. In contrast to ensembling, souping does not increase the computational cost of inference. They find that soups have better generalisation than the best individual model.

\citet{soups} hypothesise that souping works because the different training trajectories lead to models that lie in the same low-loss basin. By averaging the weights of these models, the soup remains in this low-loss basin while reducing variance. They find that when the angle formed by the pre-trained model and the two models to be souped is larger, the soup is more likely to be effective. A wider angle may suggest that fine-tuned trajectories are more divergent, thus more variance is reduced by averaging.

\subsection{Related Work}

\textbf{Souping}: Souping has been used in a variety of settings.~\citet{seasoningsoups} soup models trained to be robust to different distribution shifts and~\citet{ratatouille} soup ingredients trained on different tasks. Both cases lead to better generalisation.~\citet{modelstock} further explore the optimal soup between just two fine-tuned models by considering the angle formed by the two training trajectories in a layer-wise fashion.

\textbf{Stochastic Weight Averaging (SWA)}: Souping is related to the idea of SWA~\cite{stochasticweightaveraging}. In SWA, the ingredients of the soup come from different steps along the same training trajectory. By contrast, souping averages models from different training trajectories.

% The idea is that SGD introduces noise around a `true' minima which causes the model to bounce around a low loss basin. If we average from multiple points along this trajectory, we lower variance and find a model closer to the true minima.

\textbf{Stability Analysis}: Souping requires models to be `compatible' in the sense that averaging their weights has low loss. They define the concept of \emph{stability to SGD noise}, whereby at some point during training, models become robust to the noise introduced by SGD. They are robust in the sense that all minima obtained by training from that point onwards are linearly mode connected. This suggests that if models are trained for long enough, they will become compatible and thus souping will be effective. 

% ~\citet{linearmode} characterise such behaviour as \emph{linear mode connectivity}.

% \textbf{Permutation Alignment}: Ideas around low loss basins have also been exploited by~\citet{gitrebasin} in order to permute the neurones of models such that they are as closely aligned as possible. This is motivated from the observation that neurones can be permuted while leaving the function of the network unchanged. Therefore, two models may not be linearly mode connected, but after alignment, they may become so.

\subsection{Our Contributions}

Following these works, we seek to further our understanding of souping. We conduct a series of experiments attempting to answer the following questions:

\textbf{1. How much shared training is required for souping to be effective?} Models trained from the same pre-trained model can soup effectively, but how much of this shared training is required? We investigate this by varying the amount of shared training between models before they diverge into different training trajectories.\\
\textbf{2. Can we predict the effectiveness of souping using similarity measures?} If two models are very similar, it may be more likely that they can soup effectively. We investigate a variety of similarity measures between models and their ability to predict soup gain.\\
% \textbf{3. Does permuting models prior to souping make success more likely?} We use permutation alignment methods from~\citet{gitrebasin} to align models prior to souping and investigate whether this increases the effectiveness of souping.\\
\textbf{3. Is souping transitive?} If model A soups well with model B, and model B soups well with model C, does model A soup well with model C? The low-loss basin hypothesis suggests that this should be the case.\\
\textbf{4. Does souping help with corrupted data?} While souping has been shown to help with robustness to distribution shifts, we seek to answer how correlated the soup gains are between in-distribution and out-of-distribution data.

A further experiment investigating permuting models prior to souping can be found in Appendix~\ref{appendix:permutation_alignment}.

\subsection{Soups and Soup Gain}

For this work, we only soup pairs of models $\theta_A$ and $\theta_B$ using the simple arithmetic mean. That is, $\theta_{\text{soup}} = \frac{1}{2}\theta_A + \frac{1}{2}\theta_B$. We do not consider any other weighted average in order to save on computational cost. This decision can be justified using evidence from~\citet{gitrebasin}, where many of the loss barriers they find have the most extreme behaviour at the midpoint. Thus we assume that when souping between two models is beneficial, it will beneficial at the midpoint. When souping increases the loss, it will increase the loss at the midpoint. Thus, only using the midpoint is unlikely to miss any important effects in our experiments.

We define the \emph{soup gain} of a pair of models $\theta_A,\theta_B$ as the decrease in loss on the test set obtained by souping the models. That is, $\text{soup gain} = \min\{L(\theta_A), L(\theta_B)\} - L(\theta_{\text{soup}})$ where $L(\theta)$ denotes the test loss obtained using model $\theta$. We use the soup gain as the primary measure of the effectiveness of souping in our experiments. The gain can also be computed in terms of accuracy. This contrasts with the more lenient measure of comparing the performance of the soup with a weighted average of its ingredients, as used by~\citet{soups}. We prefer to compare to the minimum, as the purpose of a soup should be to improve over its ingredients, not to be better than than their average performance.