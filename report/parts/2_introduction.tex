\section{Introduction}

\subsection{Motivation}

~\citet{soups} introduced the idea of \emph{souping}, where models weights are averaged into a single model. Ingredients for the soup are trained from a shared pre-trained model but undergo different training trajectories. For example, models can be trained with different hyperparameters or with different data. This serves as an alternative to conventional ensembles, as souping does not increase the computational or memory cost at inference time. They find that soups can produce better minima than selecting the best individual model on a held-out validation set. Given the ease with which souping can be implemented and there being no increase in computational cost, souping has been used in a variety of settings. ~\citet{seasoningsoups} soup models trained to be robust to different distribution shifts and~\citet{ratatouille} soup ingredients trained on different tasks. Both cases lead to better generalisation.~\citet{modelstock} further explore the optimal soup between just two fine-tuned models by layer-wise averaging weights that lead to the central angle across both training trajectories.

~\citet{soups} hypothesise that souping works because the different training trajectories lead to models that lie in the same low-loss basin. By averaging the weights of these models, the soup remains in this low-loss basin while reducing variance. They find that when the angle formed by the pre-trained model and the two models to be souped is larger, the soup is more likely to be effective. A wider angle may suggest that fine-tuned trajectories are more divergent, thus more variance is reduced by averaging.


\subsection{Related Work}

\textbf{Stochastic Weight Averaging (SWA)}: Souping is related to the idea of SWA~\cite{stochasticweightaveraging}. In SWA, the ingredients of the soup come from different steps along the same training trajectory. The idea is that SGD introduces noise around a `true' minima which causes the model to bounce around a low loss basin. If we average from multiple points along this trajectory, we lower variance and find a model closer to the true minima. By contrast, souping averages models from different training trajectories. Ostensibly, this makes the noise across model weights averaged less correlated.

\textbf{Linear Mode Connectivity}: Souping requires models to be `compatible' in the sense that averaging their weights has low loss.~\citet{linearmode} characterise such behaviour as \emph{linear mode connectivity}. They further define the concept of \emph{stability to SGD noise}, whereby at some point during training, models become robust to the noise introduced by SGD. They are robust in the sense that all minima obtained by training from that point onwards are linearly mode connected. This suggests that if models are trained long enough, they will become compatible and thus souping will be effective. 

\textbf{Permutation Alignment}: Ideas around low loss basins have also been exploited by~\citet{gitrebasin} in order to permute the neurones of models such that they are as closely aligned as possible. This is motivated from the observation that neurones can be permuted while leaving the function of the network unchanged. Therefore, two models may not be linearly mode connected, but after alignment, they may become so.

\subsection{Our Contributions}

Following these works, we seek to further our understanding of souping. We conduct a series of experiments attempting to answer the following questions:
\begin{itemize}
    \item \textbf{How much shared training is required for souping to be effective?} Models trained from the same pre-trained model can soup effectively, but how much of this shared training is required? We investigate this by varying the amount of shared training between models before they diverge into different training trajectories.
    \item \textbf{Can we predict the effectiveness of souping using similarity measures?} If two models are very similar, it may be more likely that they can soup effectively. We investigate a variety of similarity measures between models and their ability to predict soup gain.
    \item \textbf{Does permuting models prior to souping make success more likely?} We use permutation alignment methods from~\citet{gitrebasin} to align models prior to souping and investigate whether this increases the effectiveness of souping.
    \item \textbf{Is souping transitive?} If model A soups well with model B, and model B soups well with model C, does model A soup well with model C? We investigate this by constructing `soup chains' of models trained from a shared pre-trained model. The low-loss basin hypothesis suggests that this should be the case.
    \item \textbf{Does souping help with out-of-distribution data?} While souping has been shown to help with robustness to distribution shifts, we seek to answer how correlated the soup gains are between in-distribution and out-of-distribution data.
\end{itemize}


\subsection{Soups and Soup Gain}

For this work, we consider the soup of models $\theta_A$ and $\theta_B$ as the simple mean of the two. That is,\

\[\theta_{\text{soup}} = \frac{\theta_A + \theta_B}{2}\] 

We do not consider any other weighted average in order to save on computational cost. This decision can be justified using evidence from~\citet{gitrebasin}, where many of the loss barriers they find have the most extreme behaviour at the midpoint. Thus we assume that when souping between two models is beneficial, it will beneficial at the midpoint. When souping increases the loss, it will increase the loss at the midpoint. Thus, only using the midpoint is unlikely to miss any important effects in our experiments.

We define the \emph{soup gain} of a pair of models $\theta_A,\theta_B$ as the decrease in loss on the test set obtained by souping the models. That is,

\[\text{soup gain} = \min\{L(\theta_A), L(\theta_B)\} - L(\theta_{\text{soup}})\]

where $L(\theta)$ denotes the test loss obtained using model $\theta$. We use the soup gain as the primary measure of the effectiveness of souping in our experiments. This contrasts with the more lenient measure of comparing the loss of the soup with a weighted average of its ingredients' performances, as used by~\citet{soups}. We prefer to compare to the minimum, as the purpose of a soup should be to improve over its ingredients, not to be better than than their average performance.