
% Title
\twocolumn[
\icmltitle{When is Model Souping Effective? Similarity, Transitivity, and Robustness}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Pierre Mackenzie}{equal}
\icmlauthor{Simon Ghyselincks}{equal}
\end{icmlauthorlist}

\printAffiliationsAndNotice{
Code available at \url{https://github.com/yourname/modelsoups}
}

% \icmlaffiliation{UBC}{Department of Computer Science, University of British Columbia, Vancouver, Canada}

\icmlcorrespondingauthor{Pierre Lardet}{pierrerl@cs.ubca.ca}
\icmlcorrespondingauthor{Simon Ghyselincks}{}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\begin{abstract}
Model souping is a technique in which the parameters of models are averaged, often leading to improved performance over constituent models without increasing inference cost. However, the specific conditions required for success are not well understood, particularly regarding the trade-off between model diversity and stability. We present a series of experiments on over 5,000 binary ResNet-50 soups trained on CIFAR-100, where we systematically vary the number of shared training epochs to control trajectory divergence. We find that effective souping requires a specific balance: models must be similar enough to avoid model collapse, but dissimilar enough to yield meaningful improvements. We also find that souping is largely transitive, providing empirical support for the hypothesis that souping works by averaging within connected low-loss basins. Finally, we observe that soup gains on corrupted data are strongly correlated with those on in-distribution data. Our findings provide insight into the mechanisms behind souping and offer practical advice for selecting model ingredients. Code and experiments are available at: \url{https://github.com/chipnbits/too-salty}.
\end{abstract}