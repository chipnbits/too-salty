
% Title
\twocolumn[
\icmltitle{Too Salty: On Model Soups}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Pierre Mackenzie}{equal}
\icmlauthor{Simon Ghyselincks}{equal}
\end{icmlauthorlist}

% \icmlaffiliation{UBC}{Department of Computer Science, University of British Columbia, Vancouver, Canada}

\icmlcorrespondingauthor{Pierre Lardet}{pierrerl@cs.ubca.ca}
\icmlcorrespondingauthor{Simon Ghyselincks}{}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\begin{abstract}
    Model souping is a technique in which the parameter weights of multiple models are averaged, often leading to improved performance over the constituent models, without increasing inference cost. However, little is understood about how souping works and when it is most effective. We present a series of experiments on over 5,000 binary Res-Net-50 soups trained on CIFAR-100 image classification. We find that souped models must be similar enough to avoid model collapse, but dissimilar enough to yield meaningful improvements. We also find that souping is mostly transitive, supporting the hypothesis that souping works by averaging within low-loss basins. Finally, we observe that soup gains on corrupted data are correlated with those on in-distribution data. Our findings provide insight into the mechanisms behind souping and offer practical guidelines for its application.
\end{abstract}
