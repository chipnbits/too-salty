
% Title
\twocolumn[
\icmltitle{Too Salty: On Model Soups}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Pierre Mackenzie}{equal}
\icmlauthor{Simon Ghyselincks}{equal}
\end{icmlauthorlist}

% \icmlaffiliation{UBC}{Department of Computer Science, University of British Columbia, Vancouver, Canada}

\icmlcorrespondingauthor{Pierre Lardet}{pierrerl@cs.ubca.ca}
\icmlcorrespondingauthor{Simon Ghyselincks}{}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\begin{abstract}
    Model souping is a technique in which the weights of multiple models are averaged, leading to improved performance without increasing inference cost. However, little is understood about how souping works and when it is most effective. In this work, we conduct a series of experiments to further our understanding. We test over 5,000 binary Res-Net-50 soups on CIFAR-100. We find that souped models must be similar enough to avoid model collapse, but dissimilar enough to yield meaningful improvements. We also find that souping is mostly transitive, supporting the hypothesis that souping works by averaging within low-loss basins. Finally, we observe that soup gains on corrupted data are correlated with those on in-distribution data. Our findings provide insight into the mechanisms behind souping and offer practical guidelines for its application.
\end{abstract}
