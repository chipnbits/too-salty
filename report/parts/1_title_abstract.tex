
% Title - Using Title Case as required
\title{When is Model Souping Effective? Similarity, Transitivity, and Robustness}

% Authors - Anonymized for review
% Note: The 'anonymous' option in documentclass will handle anonymization
% When ready for final submission, update with real names and affiliations

\author{Anonymous Authors}
\affiliation{
  \institution{Anonymous Institution}
}

% Keywords
\keywords{Machine Learning, Model Souping}

\begin{abstract}
Model souping is a post-training update technique in which the parameters of models are averaged, often leading to improved performance over constituent models without increasing inference cost. However, the specific conditions required for success are not well understood, particularly regarding the trade-off between model diversity and stability. We present a systematic analysis of over 5,000 binary ResNet-50 soups trained on CIFAR-100. To precisely control the diversity of our model ingredients, we generate pairs by branching off a shared baseline training trajectory at varying epochs, creating a spectrum of similarity ranging from near-identical to fully independent. We find that effective souping requires a specific balance: models must be similar enough to avoid model collapse, but dissimilar enough to yield meaningful improvements. Furthermore, we find that souping is largely transitive, providing empirical support for the hypothesis that souping works by averaging within connected low-loss basins. Finally, we observe that soup gains on corrupted data are strongly correlated with those on in-distribution data, suggesting that souping enhances robustness alongside accuracym without requiring additional test-time compute. Our findings provide insight into the mechanisms behind souping and offer practical advice for selecting model ingredients. Code and experiments are available at: \url{https://anonymous.4open.science/r/too-salty-478E/}.
\end{abstract}

\maketitle