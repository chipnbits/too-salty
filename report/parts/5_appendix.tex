\newpage
\appendix
\onecolumn
\section{Appendix}

% Reset figure numbering for appendix
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}

\subsection{Permutation Alignment for Souping}\label{appendix:permutation_alignment}

Following the work from~\citet{gitrebasin}, we investigate whether permuting the neurons of models prior to souping increases the effectiveness of souping. We use the \texttt{rebasin}\footnote{\url{https://pypi.org/project/rebasin/}} package to align pairs of models before souping. This package uses the `matching weights' method which permutes the neurones by inspecting only the weights. This contrasts with `activation matching' which requires forward passes through the network, and `straight through estimators' which are even more computationally expensive. The authors find that matching weights performs similarly to activation matching while being computationally cheaper. Therefore, we only consider the matching weights method.

We align all $5,346$ pairs of models using \texttt{rebasin} and compute the soup gain after alignment. Prior to permutation, $14.25\%$ of soups were positive, while after permutation, $14.32\%$ of soups were positive. However, $7\%$ of soups obtained a loss higher than $5$, which is worse than the loss of any previous soup. We plot the cumulative distribution function (CDF) of the difference in soup gain before and after permutation in Figure~\ref{fig:permutated_vs_soup_gain_cdf}. This plot shows that while permuting can sometimes help, it does not do so consistently. Further, the median difference in soup gain is approximately zero, indicating that permuting does not have a significant effect on the effectiveness of souping in our experiments. There also remains a significant risk of severe degradation. Thus, we conclude that `matching weights' permutation does not make a noticeable different to soupability in our setting.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{./figures/permutated_vs_soup_gain_cdf.pdf}
    \caption{Cumulative distribution function (CDF) of the difference in soup gain before and after permutation alignment using \texttt{rebasin}. This ignores the $7\%$ of soups with a loss higher than 5 after permutation as these make the plot difficult to interpret. The remaining mean and median difference is approximately zero, indicating that permutation alignment does not have a significant effect on the effectiveness of souping in our experiments. While some soups benefit from permutation alignment, others are negatively affected, leading to an overall negligible impact while there is a risk of severe degradation.}
    \label{fig:permutated_vs_soup_gain_cdf}
\end{figure}


\newpage
\subsection{Training Details for CIFAR-100 with ResNet-50}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/branching_models.drawio.pdf}
    \caption{Branching of fine-tuned models from baseline checkpoints. A single baseline model is trained, with checkpoints saved every 10 epochs. From each checkpoint, 4 variants are trained with different optimizer hyper-parameter perturbations.}\label{fig:evolution}
\end{figure}

\begin{table}[ht]
\centering


\begin{tabular}{lccc}
\toprule
Model & Learning Rate Scale & Momentum Scale & Weight Decay Scale \\
\midrule
Model 1 & 0.7073 & 1.1009 & 0.8284 \\
Model 2 & 1.2244 & 0.9247 & 1.1150 \\
Model 3 & 0.5112 & 1.0695 & 1.1099 \\
Model 4 & 0.5373 & 0.8594 & 0.9078 \\
\bottomrule
\end{tabular}
\caption{Optimizer perturbation scales applied during finetuning from the baseline ResNet-50 checkpoint on CIFAR-100. Each model scales the original SGD hyperparameters multiplicatively.}\label{tab:optimizer_perturbations}
\end{table}

\begin{table}[ht]
\centering

\begin{tabular}{lll}
\toprule
Component & Hyperparameter & Value \\
\midrule
Dataset 
& Dataset             & CIFAR-100 \\
& \# Classes          & 100 \\
& Data augmentation   & Mirroring and Padded Offset \\
& Validation split    & 5\% of training set \\
& Split seed          & 42 \\
\midrule
Model 
& Architecture        & ResNet-50 \\
& Pretrained          & No (from scratch) \\
\midrule
Optimization 
& Optimizer           & SGD (Nesterov) \\
& Initial learning rate & 0.1 \\
& Momentum            & 0.9 \\
& Weight decay        & $5 \times 10^{-4}$ \\
\midrule
Learning rate schedule
& Scheduler           & CosineAnnealingLR \\
& $T_{\max}$          & 280 epochs \\
& $\eta_{\min}$       & 0 \\
\midrule
Training 
& Epochs              & 300 \\
& Batch size          & 128 \\
& Mixed precision     & No \\
\bottomrule
\end{tabular}
\caption{Training hyperparameters for CIFAR-100 with ResNet-50.}\label{tab:hyperparams_baseline}
\end{table}

% Model 4
% LR scale: 0.5373375970039733
% momentum scale: 0.8593753558069187
% weight decay scale: 0.9078390214237956

% Model 2
% LR scale: 1.2243734389696643
% momentum scale: 0.9247063988876808
% weight decay scale: 1.1149574690169013

% Model 3
% LR scale: 0.511209360224139
% momentum scale: 1.0694829466942788
% weight decay scale: 1.1098937205919557

% Model 1
% LR scale: 0.7072871250784821
% momentum scale: 1.1009119841571535
% weight decay scale: 0.828417126581002

% Original run at https://wandb.ai/sghyseli/cifar100-resnet50/runs/7vu6zu3x?nw=nwusersghyseli

\clearpage
\subsection{Further Details on Experiments}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{./figures/soup_gain_vs_all_metrics.png}
    \caption{Soup gain vs various similarity and distance metrics between pairs of models. Each subplot shows the soup gain against one metric, with the Spearman correlation. All metrics perform similarly. We conclude that the more similar models are, the more likely souping is to not cause model collapse.}\label{fig:soup_gain_vs_all_metrics}
    \includegraphics[width=0.5\linewidth]{./figures/soup_gain_vs_all_metrics_positive_soups.png}
    \caption{Soup gain vs various similarity and distance metrics between pairs of models, for the subset of soups with positive soup gain. Each subplot shows the soup gain against one metric, with the Spearman correlation. Each metric correlates similarly with soup gain. We see that when models are very similar, soup gain is small, while more dissimilar models can yield larger soup gains.}\label{fig:soup_gain_vs_all_metrics_positive_soups}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{./figures/transitivity_min.png}
    \caption{Scatterplot of the soup gain of models $B$ and $C$ against the minimum soup gain of models $A$ with $B$ and $C$. Each point represents a triplet of models $(A, B, C)$. We observe a positive Spearman correlation of 0.64, suggesting that souping is fuzzily transitive. The correlation with the mean soup gain is lower, at 0.49.}
    \label{fig:transitivity_min}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{./figures/soup_test_vs_corrupted.png}
    \caption{Scatterplot of the soup gain on test vs corrupted data. There is a clear sub-linear trend with strong correlation. Such a close relationship is sensitive to the nature of the distribution shift. This plot mostly shows that when model collapse occurs on the original test set, it also occurs on the corrupted data.}
    \label{fig:test_vs_corrupted}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{./figures/soup_positive_gain_prob.pdf}
    \caption{Probability of positive gain for soups as a function of the number of shared epochs. We see that the probability of positive gain increases with the number of shared epochs, for both clean and corrupted data. However,the corrupted data consistently has a slightly lower probability of positive gain. Thus, while souping also helps on corrupted data, it is slightly less effective than on clean data.}
    \label{fig:soup_corrupted_positive_gain_prob}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{./figures/soup_robustness_cdf.png}
    \caption{CDF of the difference in \emph{robustness gap} before and after souping. The robustness gap is defined as the difference in loss between the test set and the corrupted set. The robustness gap before souping is taken as the minimum robustness gap of the parents. We see a fairly symmetric distribution. It has mean -0.04 and median 0.02. We therefore conclude that souping does not systematically improve the robustness gap.}
    \label{fig:soup_robustness_cdf}
\end{figure}

\subsection{Details for Model Embedding Creation}\label{sec:model_embedding_creation}

Given our evidence for transitivity, we consider the broader landscape of all $104$ of our originally trained models. Do the soups all exist in separate clusters of models in separate loss basins? We define a distance metric defined as \[d_{AB} = -\operatorname{sign}(\text{soup gain}) - 0.1 * \text{soup gain}\] 

where $d_{AB}$ is the distance between models $A$ and $B$. Intuitively, this metric puts models close together that soup together positively, taking into account the magnitude of soup gain. We then cast this down into a 2-dimensional embedding using Multidimensional Scaling. We also color by branching epoch and mark edges that represent successful soups. The resulting plot is shown in Figure~\ref{fig:model_embedding}. 