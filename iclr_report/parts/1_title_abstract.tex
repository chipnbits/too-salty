% Keywords
% \keywords{Machine Learning, Model Souping}
\maketitle

\begin{abstract}
Model souping is a post-training technique where the parameters of models are averaged, often leading to improved performance over constituent models without increasing inference cost. However, the specific conditions required for success are not well understood, particularly regarding the trade-off between model diversity and stability. We analyse over 5,000 binary ResNet-50 soups trained on CIFAR-100, with diversity controlled by branching ingredients from a shared training trajectory at varying epochs. We find that effective souping requires a balance: models must be similar enough to avoid model collapse, but diverse enough to yield improvements. Furthermore, we provide empirical evidence for the hypothesis that souping works by averaging within a low-loss basin. We also observe that soup gains on corrupted data are strongly correlated with those on in-distribution data. Our findings suggest that souping can serve as an effective mechanism for enhancing model robustness without requiring additional test-time compute but that, in many cases, it fails to beat simpler methods like Stochastic Weight Averaging (SWA). Code and experiments are available at: \url{https://anonymous.4open.science/r/too-salty-478E/}.
\end{abstract}

