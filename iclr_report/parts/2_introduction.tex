\section{Introduction}

% \subsection{Motivation}

% Fine-tuning is a fundamental form of adaptation where updating a pre-trained model on data from a new distribution can improve performance for that specific target domain. 

\citet{soups} introduce \emph{souping}: averaging the model parameters produced by different fine-tuning trajectories from a common pre-trained model can yield better generalization than any one ingredient. Souping is formulated as a linear interpolation of model \textit{parameters} $\theta$ using \textit{weights} $\alpha$; for the case of two models, $\theta_\text{soup} = (1-\alpha)\theta_A + \alpha \theta_B$, creating a new single model. This not only captures the benefit of multiple optimization paths but also introduces an additional adaptation opportunity, as \citet{seasoningsoups} suggest that dynamically adjusting the interpolation weights enables intermediate behaviours that can better match a range of distributional shifts. In contrast to ensembling which combines the \textit{outputs} of models, souping combines the \textit{parameters}, maintaining the same computational cost of inference with a single forward pass.

\citet{soups} hypothesise that souping works because fine-tuned models often lie within the same low-loss basin. A convex combination of their parameters is expected to remain within this basin while reducing the variance introduced by noisy training. They find that when the angle formed by the pre-trained model and ingredients to be souped together is larger, implying greater diversity, there is a greater performance boost from souping. Understanding why averaging maintains a low loss, how diversity contributes to robustness, and when souping is beneficial is therefore essential for studying adaptation more broadly. 

% Averaging models that have not settled into a shared basin can lead to catastrophic performance degradation or ``model collapse'' where , motivating our investigation into identifying this transition. 

\textbf{Related Work:} Souping has been used in a variety of settings. \citet{seasoningsoups} soup models trained to be robust to different distribution shifts and~\citet{ratatouille} soup ingredients trained on different tasks. Both cases lead to better generalisation.~\citet{modelstock} show that fine-tuned parameters are distributed around a high-accuracy center and use the angle between as few as two fine-tuned weights and their shared branch point to approximate this low-loss center.

Souping is related to the idea of SWA~\cite{stochasticweightaveraging}. In SWA, the ingredients of the soup come from different steps along the same training trajectory. By contrast, souping averages models from independent training trajectories from a shared initialization. \\
% \textbf{Exponential Moving Average (EMA)}: Another method for averaging weights over training is EMA~\cite{exponentialmovingaverageweights}. Here, the weights at each training step are combined with the previous average using an exponential decay.
% The idea is that SGD introduces noise around a `true' minima which causes the model to bounce around a low loss basin. If we average from multiple points along this trajectory, we lower variance and find a model closer to the true minima.

% \textbf{Stability Analysis}: Souping success requires models to be `compatible' in the sense that averaging their weights has low loss.~\citet{linearmode} define \emph{stability to SGD noise}, whereby at some point during training, models become robust to the noise from SGD in the sense that all possible minima obtained by training from that point onwards lie in the same low-loss basin.
Souping success requires models to be `compatible' in the sense that averaging their weights has low loss. This notion is closely related to `stability to SGD noise'~\citet{linearmode}, enabled by linear mode connectivity between models trained from the same initialisation under different SGD noise. Such stability is only present after sufficient shared training~\citet{transientperformancegains}. We propose that this defines window during training in which souping is effective and that pre-emptive souping leads to a high loss barrier, or `model collapse'.

% ~\citet{linearmode} characterise such behaviour as \emph{linear mode connectivity}.

% \textbf{Permutation Alignment}: Ideas around low loss basins have also been exploited by~\citet{gitrebasin} in order to permute the neurones of models such that they are as closely aligned as possible. This is motivated from the observation that neurones can be permuted while leaving the function of the network unchanged. Therefore, two models may not be linearly mode connected, but after alignment, they may become so.

\textbf{Our Contributions:} Following these works, we seek to better understand souping by conducting a series of experiments addressing the following questions:

\textbf{How much shared training is required for souping to be effective?} We investigate varying the number of shared pre-training epochs before splitting into fine-tuned variants to empirically map the transition from model collapse to effective souping.\\
%  Models trained from the same pre-trained checkpoint can soup effectively, but how much shared training is required?\\We investigate varying the number of shared pre-training epochs before splitting into fine-tuned variants.
\textbf{Can model similarity predict the effectiveness of souping?} Identical models yield no generalization benefit, while overly dissimilar models fail to inhabit a shared low-loss region. We investigate whether standard similarity metrics can predict this balance between model compatibility and diversity.\\
% If two models are very similar, it may be more likely that they can soup effectively.\\
% We investigate a variety of similarity measures between models and their ability to predict soup gain
% \textbf{3. Does permuting models prior to souping make success more likely?} We use permutation alignment methods from~\citet{gitrebasin} to align models prior to souping and investigate whether this increases the effectiveness of souping.\\
\textbf{Is souping transitive?} If model $A$ soups with $B$, and $B$ soups with $C$, will $A$ soup with $C$? \\
% The low-loss basin hypothesis suggests that this should be the case.
\textbf{Does souping in-distribution predict souping out-of-distribution?} While souping has been shown to help with robustness to distribution shifts, we seek to answer how correlated the soup gains are between in-distribution and out-of-distribution data.

More experiments on the effect of permuting models~\cite{gitrebasin} prior to souping and how souping affects robustness can be found in Appendix~\ref{appendix:permutation_alignment} and Figure~\ref{fig:soup_robustness_cdf} respectively.

\textbf{Soups and Soup Gain:} We soup pairs of models using the simple arithmetic mean $\theta_{\text{soup}} = \frac{1}{2}\theta_A + \frac{1}{2}\theta_B$. As shown by~\citet{gitrebasin} and~\cite{transientperformancegains}, the loss barrier typically has the most extreme behaviour at the midpoint, making it a useful summary statistic of souping performance across weights interpolation, accurately identifying souping failure. Characteristic weighting plots are shown in \ref{fig:soup_weghts}. 

\emph{Soup gain} is the test set accuracy gain of souping two models relative to the \emph{best} parent model. 
$$\text{soup gain} = \text{acc}(\theta_{\text{soup}}) - \max\{\text{acc}(\theta_A), \text{acc}(\theta_B)\}$$
where $\text{acc}(.)$ denotes test accuracy. We compare against the maximum accuracy of the ingredients, rather than the mean, as the purpose of a soup should be to improve over its ingredients. Soup gain can alternatively be measured over the loss. We use soup gain as the primary measure of the effectiveness of souping. We also refer to soups with positive and negative soup gain simply as positive and negative soups. 